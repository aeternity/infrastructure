- name: Collect cloud instance metadata
  hosts: all
  remote_user: master
  become: true
  gather_facts: true
  tags: [datadog]
  vars:
    enable_aws_checks: true
  tasks:
    - name: Check if running on AWS instance
      ansible.builtin.uri:
        url: http://169.254.169.254/latest/meta-data
        timeout: 2
      register: aws_uri_check
      failed_when: false
      when:
        - enable_aws_checks|bool
        - packer_build_name is not defined

    - name: Set AWS check fact
      ansible.builtin.set_fact:
        is_aws: "{{ packer_build_name is not defined and aws_uri_check.status == 200 }}"
      when:
        - enable_aws_checks|bool

    - name: Get instance metadata facts
      amazon.aws.ec2_metadata_facts:
      when:
        - enable_aws_checks|bool
        - is_aws

- name: Configure health check services (goss)
  hosts: all
  remote_user: master
  become: true
  vars:
    project_user: aeternity
    goss_version: "0.3.6"

  tasks:
    - name: Create Goss source directory
      ansible.builtin.file:
        path: /usr/src/goss-{{ goss_version }}/
        state: directory
      tags:
        - goss-install

    - name: Download and unarchive Goss binary
      ansible.builtin.get_url:
        url: "https://github.com/aelsabbahy/goss/releases/download/v{{ goss_version }}/goss-linux-amd64"
        dest: /usr/src/goss-{{ goss_version }}/goss
      tags:
        - goss-install

    - name: Install Goss binary
      ansible.builtin.copy:
        src: /usr/src/goss-{{ goss_version }}/goss
        dest: /usr/bin/goss
        mode: '0755'
        remote_src: true
      tags:
        - goss-install

    - name: Create Goss config directory
      ansible.builtin.file:
        path: /etc/goss/
        state: directory
      notify: restart goss
      tags:
        - goss-install

    - name: Check is systemd enabled
      ansible.builtin.stat:
        path: /run/systemd/system
      register: systemd_run_dir
      tags:
        - goss-install

    - name: Copy systemd goss config
      ansible.builtin.copy:
        src: files/goss/goss.service
        dest: "/etc/systemd/system"
        mode: '0640'
      notify: reload systemd
      when: systemd_run_dir.stat.exists and systemd_run_dir.stat.isdir
      tags:
        - goss-install

    - name: Enable goss systemd service start on boot
      ansible.builtin.systemd:
        name: goss
        enabled: true
      notify: reload systemd
      when: systemd_run_dir.stat.exists and systemd_run_dir.stat.isdir
      tags:
        - goss-install

    - name: Copy init.d goss config
      ansible.builtin.copy:
        src: files/goss/goss.conf
        dest: "/etc/init.d/goss"
        mode: '0750'
      notify: restart goss
      when: not systemd_run_dir.stat.exists
      tags:
        - goss-install

    - name: Set goss.log correct permissions for goss
      ansible.builtin.file:
        path: /var/log/goss.log
        state: touch
        mode: '0640'
        owner: "{{ project_user }}"
        modification_time: "preserve"
        access_time: "preserve"
      notify: restart goss
      tags:
        - goss-install

    - name: Copy test files
      ansible.builtin.copy:
        directory_mode: true
        src: ../test/goss/local/
        dest: "/etc/goss/"
      notify: restart goss
      tags:
        - goss-install

    - name: Copy health check script
      ansible.builtin.copy:
        src: ../scripts/health_check.sh
        dest: "/etc/goss/"
        mode: '0755'
      notify: restart goss
      tags:
        - goss-install

  handlers:
    - name: Reload systemd
      ansible.builtin.systemd:
        daemon_reload: true
      listen:
        - reload systemd
      notify: restart goss
      tags:
        - goss-install

    - name: Restart goss
      ansible.builtin.service:
        name: goss
        state: restarted
        use: service_mgr|default(ansible_service_mgr)
      listen:
        - restart goss
      tags:
        - goss-install

- name: Configure monitoring services (DataDog)
  hosts: all
  remote_user: master
  become: true
  gather_facts: false
  tags: [datadog]

  vars:
    project_user: aeternity
    public_ipv4: "{{ ansible_ec2_public_ipv4|default(ansible_ssh_host)|default(ansible_host)|default(inventory_hostname) }}"
    fail2ban_enable: true
    datadog_api_key: "{{ lookup('hashi_vault', 'secret=secret/datadog/agent:api_key') }}"
    datadog_default_tags:
      - "lsb:{{ ansible_lsb.description }}"
      - "public_ipv4:{{ public_ipv4|default('unknown') }}"
      - "network_id:{{ network_id|default('unknown') }}"
    datadog_agent_major_version: 6
    datadog_config:
      log_level: warning
      log_to_console: false
      apm_enabled: false
      use_dogstatsd: true
      process_config:
        enabled: "true" # has to be set as a string
      apm_config:
        enabled: false
      logs_enabled: true
      tags: "{{ (datadog_tags + datadog_default_tags) if datadog_tags is defined else datadog_default_tags }}"
    datadog_checks:
      system_core:
        init_config:
        instances:
          # The Agent just needs one item in instances in order to enable the check.
          # The content of the item doesnâ€™t matter.
          - foo: bar
      process:
        init_config:
        instances:
          - name: ssh
            search_string: ['sshd']
          - name: aeternity
            search_string: ['beam.smp']
          - name: epmd
            search_string: ['epmd']
            thresholds:
              warning: [1, 1]
      http_check:
        init_config:
        instances:
          - name: API
            url: "http://localhost:8080/healthz"
            # Default is false, i.e. emit events instead of service checks.
            # Recommend to set to true.
            skip_event: true
      logs_agent:
        init_config:
        instances:
        logs:
          - type: tcp
            port: 10518
            service: aeternity
            source: syslog
          - type: file
            path: "/home/{{ project_user }}/node/log/aeternity.log"
            service: aeternity
            source: lager
            sourcecategory: aeternity
          - type: file
            path: "/home/{{ project_user }}/node/log/aeternity_mining.log"
            service: aeternity
            source: lager
            sourcecategory: mining
          - type: file
            path: "/home/{{ project_user }}/node/log/aeternity_pow_cuckoo.log"
            service: aeternity
            source: lager
            sourcecategory: pow
          - type: file
            path: "/home/{{ project_user }}/node/log/aeternity_sync.log"
            service: aeternity
            source: lager
            sourcecategory: sync

  pre_tasks:
    - name: "Add dd-agent to {{ project_user ~ 'group' }}"
      ansible.builtin.user:
        name: dd-agent
        groups: "{{ project_user }}"
        append: true

  roles:
    - { role: Datadog.datadog }

  post_tasks:
    - name: Check is systemd enabled
      ansible.builtin.stat:
        path: /run/systemd/system
      register: systemd_run_dir
    - name: Install rsyslog
      ansible.builtin.apt:
        state: present
        update_cache: true
        pkg:
          - rsyslog
    - name: Copy datadog.conf to rsyslog
      ansible.builtin.copy:
        src: files/rsyslog/datadog.conf
        dest: /etc/rsyslog.d/datadog.conf
        mode: '0600'
      notify: restart rsyslogd
    - name: Copy fail2ban.conf to rsyslog
      ansible.builtin.copy:
        src: files/rsyslog/fail2ban.conf
        dest: /etc/rsyslog.d/49-fail2ban.conf
        mode: '0600'
      notify: restart rsyslogd
    - name: Copy fail2ban.conf to logrotate
      ansible.builtin.copy:
        src: files/logrotate/fail2ban
        dest: /etc/logrotate.d/fail2ban
        mode: '0600'
    - name: Set fail2ban.log correct permissions for syslog
      ansible.builtin.file:
        path: /var/log/fail2ban.log
        state: touch
        mode: '0640'
        owner: syslog
        group: adm
        modification_time: "preserve"
        access_time: "preserve"
      notify:
        - restart rsyslogd
        - restart fail2ban
    - name: Remove default fail2ban conf.d from Debian/Ubuntu
      ansible.builtin.file:
        path: /etc/fail2ban/jail.d/defaults-debian.conf
        state: absent
      notify: restart fail2ban


  handlers:
    - name: Restart rsyslogd
      ansible.builtin.service:
        name: rsyslog
        state: restarted
        use: service_mgr|default(ansible_service_mgr)
      when: systemd_run_dir.stat.exists and systemd_run_dir.stat.isdir
      listen:
        - restart rsyslogd
    - name: Restart fail2ban
      ansible.builtin.service:
        name: fail2ban
        state: restarted
        use: service_mgr|default(ansible_service_mgr)
      when: fail2ban_enable | bool
      listen:
        - restart fail2ban

- name: Configure monitoring services (Prometheus exporters)
  hosts: all
  remote_user: master
  become: true
  gather_facts: false
  tags: [prometheus_exporters]

  vars:
    node_exporter_version: '1.7.0'
    statsd_exporter_mapping_config: 'files/statsd_mapping.yaml'
    statsd_exporter_relay_address: 'localhost:8125'

  roles:
    - role: node_exporter
      when: node_exporter_enabled
    - role: statsd_exporter

- name: Configure logging services (fluent-bit)
  hosts: all
  remote_user: master
  become: true
  gather_facts: false
  tags: [fluentbit]

  vars:
    public_ipv4: "{{ ansible_ec2_public_ipv4|default(ansible_ssh_host)|default(ansible_host)|default(inventory_hostname) }}"
    loki_http_user: "{{ lookup('hashi_vault', 'secret=secret/loki/http_auth:user') }}"
    loki_http_password: "{{ lookup('hashi_vault', 'secret=secret/loki/http_auth:password') }}"
    fluentbit_pkg_version: 3.0.7
    fluentbit_dbs_path: /var/lib/flb-storage
    fluentbit_svc_storage:
      path: /var/lib/flb-storage/
      sync: full
      checksum: "off"
    fluentbit_systemd_input:
      - name: systemd
        tag: systemd.*
        lowercase: "on"
        strip_underscores: "on"
        read_from_tail: "on"
    fluentbit_aenode_input:
      - name: tail
        tag: aenode.*
        path: /home/aeternity/node/log/*.log
        path_key: filepath
    fluentbit_records_filter:
      # - name: record_modifier
      #   match: systemd.*
      #   record: job fluentbit.host.systemd
      # - name: record_modifier
      #   match: aenode.*
      #   record: job fluentbit.host.aenode
      - name: record_modifier
        match: "*"
        record: "public_ipv4 {{ public_ipv4 }}"
    fluentbit_aws_filter:
      - name: aws
        match: '*'
        imds_version: v2
        az: "true"
        ec2_instance_id: "true"
        ec2_instance_type: "true"
        private_ip: "true"
        ami_id: "true"
        hostname: "true"
        tags_enabled: "true"
        tags_include: Name,role,env,kind,color
    fluentbit_loki_systemd_output:
      - name: loki
        match: '*'
        host: loki.dev.aeternity.io
        port: 443
        tls: "on"
        http_user: "{{ loki_http_user }}"
        http_passwd: "{{ loki_http_password }}"
        labels: job=fluentbit.host.systemd
        label_keys: $role, $env, $kind, $ec2_instance_id, $systemd_unit, $priority
    fluentbit_loki_aenode_output:
      - name: loki
        match: '*'
        host: loki.dev.aeternity.io
        port: 443
        tls: "on"
        http_user: "{{ loki_http_user }}"
        http_passwd: "{{ loki_http_password }}"
        labels: job=fluentbit.host.aenode
        label_keys: $role, $env, $kind, $ec2_instance_id

  roles:
    - role: fluentbit
