- name: Collect cloud instance metadata
  hosts: all
  remote_user: master
  become: yes
  gather_facts: yes
  tags: [datadog]
  tasks:
    - name: Check if running on AWS instance
      uri:
        url: http://169.254.169.254/latest/meta-data
        timeout: 2
      register: aws_uri_check
      failed_when: False
      when: packer_build_name is not defined

    - name: Set AWS check fact
      set_fact:
        is_aws: "{{ packer_build_name is not defined and aws_uri_check.status == 200 }}"

    - name: Get instance metadata facts
      ec2_metadata_facts:
      when: is_aws

    - name: "Load a variable files for environment: {{ env }}"
      include_vars: "{{ item }}"
      with_first_found:
        - files:
            - "vars/aeternity/{{ env }}.yml"
          skip: true

- name: Configure health check services (goss)
  hosts: all
  remote_user: master
  become: yes
  vars:
    project_user: aeternity
    goss_version: "0.3.6"

  tasks:
    - name: Create Goss source directory
      file:
        path: /usr/src/goss-{{ goss_version }}/
        state: directory
      tags:
        - goss-install

    - name: Download and unarchive Goss binary
      get_url:
        url: "https://github.com/aelsabbahy/goss/releases/download/v{{ goss_version }}/goss-linux-amd64"
        dest: /usr/src/goss-{{ goss_version }}/goss
      tags:
        - goss-install

    - name: Install Goss binary
      copy:
        src: /usr/src/goss-{{ goss_version }}/goss
        dest: /usr/bin/goss
        mode: 0755
        remote_src: true
      tags:
        - goss-install

    - name: Create Goss config directory
      file:
        path: /etc/goss/
        state: directory
      notify: restart goss
      tags:
        - goss-install

    - name: Check is systemd enabled
      stat:
        path: /run/systemd/system
      register: systemd_run_dir
      tags:
        - goss-install

    - name: Copy systemd goss config
      copy:
        src: files/goss/goss.service
        dest: "/etc/systemd/system"
        mode: 0640
      notify: reload systemd
      when: systemd_run_dir.stat.exists and systemd_run_dir.stat.isdir
      tags:
        - goss-install

    - name: Enable goss systemd service start on boot
      systemd:
        name: goss
        enabled: yes
      notify: reload systemd
      when: systemd_run_dir.stat.exists and systemd_run_dir.stat.isdir
      tags:
        - goss-install

    - name: Copy init.d goss config
      copy:
        src: files/goss/goss.conf
        dest: "/etc/init.d/goss"
        mode: 0750
      notify: restart goss
      when: not systemd_run_dir.stat.exists
      tags:
        - goss-install

    - name: Set goss.log correct permissions for goss
      file:
        path: /var/log/goss.log
        state: touch
        mode: 0640
        owner: "{{ project_user }}"
        modification_time: "preserve"
        access_time: "preserve"
      notify: restart goss
      tags:
        - goss-install

    - name: Copy test files
      copy:
        directory_mode: true
        src: ../test/goss/local/
        dest: "/etc/goss/"
      notify: restart goss
      tags:
        - goss-install

    - name: Copy health check script
      copy:
        src: ../scripts/health_check.sh
        dest: "/etc/goss/"
        mode: 0755
      notify: restart goss
      tags:
        - goss-install

  handlers:
    - name: Reload systemd
      systemd:
        daemon_reload: yes
      listen:
        - reload systemd
      notify: restart goss
      tags:
        - goss-install

    - name: Restart goss
      service:
        name: goss
        state: restarted
      listen:
        - restart goss
      tags:
        - goss-install
    
    - name: restart statsd_exporter
      become: true
      systemd:
        daemon_reload: true
        name: statsd_exporter
        state: restarted


- name: Configure monitoring services (DataDog)
  hosts: all
  remote_user: master
  become: yes
  gather_facts: no
  tags: [datadog]

  vars:
    project_user: aeternity
    public_ipv4: "{{ ansible_ec2_public_ipv4|default(ansible_ssh_host)|default(ansible_host)|default(inventory_hostname) }}"
    datadog_api_key: "{{ lookup('hashi_vault', 'secret=secret/datadog/agent:api_key') }}"
    network_id: "{{ (node_config['fork_management']|default(dict(network_id = 'unknown')))['network_id'] }}"
    datadog_default_tags:
      - "lsb:{{ ansible_lsb.description }}"
      - "public_ipv4:{{ public_ipv4|default('unknown') }}"
      - "network_id:{{ network_id }}"
    datadog_agent_major_version: 6
    datadog_config:
      log_level: warning
      log_to_console: false
      apm_enabled: false
      use_dogstatsd: true
      process_config:
        enabled: "true" # has to be set as a string
      apm_config:
        enabled: false
      logs_enabled: true
      tags: "{{ (datadog_tags + datadog_default_tags) if datadog_tags is defined else datadog_default_tags }}"
    datadog_checks:
      system_core:
        init_config:
        instances:
          # The Agent just needs one item in instances in order to enable the check.
          # The content of the item doesnâ€™t matter.
          - foo: bar
      process:
        init_config:
        instances:
          - name: ssh
            search_string: ['sshd']
          - name: aeternity
            search_string: ['beam.smp']
          - name: epmd
            search_string: ['epmd']
            thresholds:
              warning: [1, 1]
      http_check:
        init_config:
        instances:
          - name: API
            url: "http://localhost:8080/healthz"
            # Default is false, i.e. emit events instead of service checks.
            # Recommend to set to true.
            skip_event: true
      logs_agent:
        init_config:
        instances:
        logs:
          - type: tcp
            port: 10518
            service: aeternity
            source: syslog
          - type: file
            path: "/home/{{ project_user }}/node/log/aeternity.log"
            service: aeternity
            source: lager
            sourcecategory: aeternity
          - type: file
            path: "/home/{{ project_user }}/node/log/aeternity_mining.log"
            service: aeternity
            source: lager
            sourcecategory: mining
          - type: file
            path: "/home/{{ project_user }}/node/log/aeternity_pow_cuckoo.log"
            service: aeternity
            source: lager
            sourcecategory: pow
          - type: file
            path: "/home/{{ project_user }}/node/log/aeternity_sync.log"
            service: aeternity
            source: lager
            sourcecategory: sync

  pre_tasks:
    - name: "Add dd-agent to {{ project_user }} group"
      user:
        name: dd-agent
        groups: "{{ project_user }}"
        append: yes

  roles:
    - { role: Datadog.datadog }

  post_tasks:
    - name: Install rsyslog
      apt:
        state: present
        update_cache: yes
        pkg:
        - rsyslog
    - name: Copy datadog.conf to rsyslog
      copy:
        src: files/rsyslog/datadog.conf
        dest: /etc/rsyslog.d/datadog.conf
        mode: 0600
      notify: restart rsyslogd
    - name: Copy fail2ban.conf to rsyslog
      copy:
        src: files/rsyslog/fail2ban.conf
        dest: /etc/rsyslog.d/49-fail2ban.conf
        mode: 0600
      notify: restart rsyslogd
    - name: Copy fail2ban.conf to logrotate
      copy:
        src: files/logrotate/fail2ban
        dest: /etc/logrotate.d/fail2ban
        mode: 0600
    - name: Set fail2ban.log correct permissions for syslog
      file:
        path: /var/log/fail2ban.log
        state: touch
        mode: 0640
        owner: syslog
        group: adm
        modification_time: "preserve"
        access_time: "preserve"
      notify:
        - restart rsyslogd
        - restart fail2ban
    - name: Remove default fail2ban conf.d from Debian/Ubuntu
      file:
        path: /etc/fail2ban/jail.d/defaults-debian.conf
        state: absent
      notify: restart fail2ban

  handlers:
    - name: Restart rsyslogd
      service:
        name: rsyslog
        state: restarted
      listen:
        - restart rsyslogd
    - name: Restart fail2ban
      service:
        name: fail2ban
        state: restarted
      listen:
        - restart fail2ban

#### start statsd here
- name: Configure statsd exporter
  hosts: all
  remote_user: master
  become: yes
  vars:
    statsd_exporter_version: 0.23.0
    statsd_exporter_web_listen_address: ":9102"
    statsd_exporter_listen_udp: ":8125"
    statsd_exporter_listen_tcp: ":8125"
    statsd_architecture: "x86_64"

  tasks:
    - name: Naive assertion of proper listen address
      assert:
        that:
          - "':' in statsd_exporter_web_listen_address"

    - block:
        - name: Get latest release
          uri:
            url: "https://api.github.com/repos/prometheus/statsd_exporter/releases/latest"
            method: GET
            return_content: true
            status_code: 200
            body_format: json
            validate_certs: false
            user: "{{ lookup('env', 'GH_USER') | default(omit) }}"
            password: "{{ lookup('env', 'GH_TOKEN') | default(omit) }}"
          no_log: true
          register: _latest_release
          until: _latest_release.status == 200
          retries: 5

        - name: "Set statsd_exporter version to {{ _latest_release.json.tag_name[1:] }}"
          set_fact:
            statsd_exporter_version: "{{ _latest_release.json.tag_name[1:] }}"
      when: statsd_exporter_version == "latest"

    - name: "Get checksum for {{ statsd_architecture }} architecture"
      set_fact:
        statsd_exporter_checksum: "{{ item.split(' ')[0] }}"
      with_items:
        - "{{ lookup('url', 'https://github.com/prometheus/statsd_exporter/releases/download/v' + statsd_exporter_version + '/sha256sums.txt', wantlist=True) | list }}"
      when: "('linux-' + (statsd_architecture + '.tar.gz') in item"

    - name: Get systemd version
      shell: systemctl --version | awk '$1 == "systemd" {print $2}'
      changed_when: false
      check_mode: false
      register: statsd_exporter_systemd_version
      tags:
        - skip_ansible_lint

    - name: Install dependencies
      package:
        name: "{{ item }}"
        state: present
      with_items: "{{ statsd_exporter_dependencies }}"

    - name: Create the statsd_exporter group
      group:
        name: statsd
        state: present
        system: true

    - name: Create the statsd_exporter user
      user:
        name: statsd
        groups: statsd
        append: true
        shell: /usr/sbin/nologin
        system: true
        createhome: false
        home: /

    - name: Download statsd_exporter binary to local folder
      become: false
      get_url:
        url: "https://github.com/prometheus/statsd_exporter/releases/download/v{{ statsd_exporter_version }}/statsd_exporter-{{ statsd_exporter_version }}.linux-{{ statsd_architecture }}.tar.gz"
        dest: "/tmp/statsd_exporter-{{ statsd_exporter_version }}.linux-{{ statsd_architecture }}.tar.gz"
        checksum: "sha256:{{ statsd_exporter_checksum }}"
      register: _download_binary
      until: _download_binary is succeeded
      retries: 5
      delay: 2
      delegate_to: localhost
      check_mode: false

    - name: Unpack statsd_exporter binary
      become: false
      unarchive:
        src: "/tmp/statsd_exporter-{{ statsd_exporter_version }}.linux-{{ statsd_architecture }}.tar.gz"
        dest: "/tmp"
        creates: "/tmp/statsd_exporter-{{ statsd_exporter_version }}.linux-{{ statsd_architecture }}/statsd_exporter"
      delegate_to: localhost
      check_mode: false

    - name: Propagate statsd_exporter binaries
      copy:
        src: "/tmp/statsd_exporter-{{ statsd_exporter_version }}.linux-{{ statsd_architecture }}/statsd_exporter"
        dest: "/usr/local/bin/statsd_exporter"
        mode: 0750
        owner: statsd
        group: statsd
      notify: restart statsd_exporter
      when: not ansible_check_mode

    - name: Copy the statsd_exporter systemd service file
      template:
        src: statsd_exporter.service.j2
        dest: /etc/systemd/system/statsd_exporter.service
        owner: root
        group: root
        mode: 0644
      notify: restart statsd_exporter
